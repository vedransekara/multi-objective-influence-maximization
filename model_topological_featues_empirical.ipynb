{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build simple ML model to understand vulnerabilities in empirical networks\n",
    "\n",
    "Use logistic regression to build classification model of nodes vulnerabilities (better off in all model: 0, worse off in all models: 1). Features are network measures:\n",
    "* degree\n",
    "* average neighbor degree\n",
    "* clustering\n",
    "* k-shell\n",
    "* betweeness centrality\n",
    "* eigen vector centrality\n",
    "* closeness centrality\n",
    "* eccentricity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from core_functions import load_network\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "#rc('text', usetex=True)\n",
    "plt.rcParams['pdf.fonttype'] = 'truetype'\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = ['ia-infect-dublin','polblogs','UC_irvine','facebook_combined','ca-AstroPh']\n",
    "network_path = '/Volumes/ExtremeSSD/last-mile/last_mile/networks_empirical/'\n",
    "\n",
    "path_to_activation_count_data = 'some_path'\n",
    "path_to_activation_time_data = 'some_path'\n",
    "\n",
    "# pick which type of vulnerability to run models for\n",
    "frequency = True # to run models for frequency\n",
    "#frequency = False # to run models for recency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do some precomputing of centrality measures for networks as they can be expensive to continually recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first pre-compute centrality metrics for a network and save the files - this speeds things up when doing the ML partdef load_measure(id_,type_):\n",
    "save_path = 'some_path/'\n",
    "for network_id in networks:\n",
    "    G = load_network(network_path + network_id + '.txt')\n",
    "    \n",
    "    # betweenness centrality\n",
    "    C = nx.betweenness_centrality(G)\n",
    "    pickle.dump(C, open(save_path + '%s_betweenness.pkl' % network_id,'wb'))\n",
    "\n",
    "    # closeness centrality\n",
    "    C = nx.closeness_centrality(G)\n",
    "    pickle.dump(C, open(save_path + '%s_closeness.pkl' % network_id,'wb'))\n",
    "\n",
    "    # eccentricity\n",
    "    C = nx.eccentricity(G)\n",
    "    pickle.dump(C, open(save_path + '%s_eccentricity.pkl' % network_id,'wb'))\n",
    "\n",
    "    # eigenvector centrality\n",
    "    C = nx.eigenvector_centrality(G)\n",
    "    pickle.dump(C, open(save_path + '%s_eigenvector.pkl' % network_id,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to later load the saved centrality files\n",
    "def load_measure(save_path, network_id, centrality_type):\n",
    "    pkl_file = open(save_path + '%s_%s.pkl' % (network_id,centrality_type),'rb')\n",
    "    #pkl_file = open('some_path/%s_%s.pkl' % (network_id,centrality_type),'rb')\n",
    "    centrality = pickle.load(pkl_file, encoding='latin1')\n",
    "    pkl_file.close()\n",
    "\n",
    "    return centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oprerationalize for multiple networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_nodes(activation_count_,activation_time_):\n",
    "    '''\n",
    "    Label nodes, with \n",
    "    0 : better off in all models\n",
    "    1 : worse off in all models\n",
    "    '''\n",
    "    \n",
    "    methods = ['coreHD', 'degreeDiscount', 'kcore', 'HD']\n",
    "\n",
    "    # counter to count how many nods are worse off\n",
    "    worse_off_freq = Counter()\n",
    "    # go through methods of selecting seeds - {random}\n",
    "    for method in methods:\n",
    "        worse_off_freq += Counter([n for n in activation_count_['random'] if activation_count_[method].get(n,0) < activation_count_['random'][n]])\n",
    "\n",
    "    # counter to count how many nods are worse off\n",
    "    worse_off_rec = Counter()\n",
    "    # go through methods of selecting seeds - {random}\n",
    "    for method in methods:\n",
    "        worse_off_rec += Counter([n for n in activation_time_['random'] if activation_time_[method].get(n,0) < activation_time_['random'][n]])\n",
    "\n",
    "    # infer nodes to include in ML dataset \n",
    "    label_freq = dict()\n",
    "    label_rec = dict()\n",
    "    for n in G.nodes():\n",
    "        \n",
    "        if n not in worse_off_freq:\n",
    "            label_freq[n] = 0 # node always is better off\n",
    "        if worse_off_freq[n] == 4:\n",
    "            label_freq[n] = 1 # if node always is worse off\n",
    "\n",
    "        if n not in worse_off_rec:\n",
    "            label_rec[n] = 0 # node always is better off\n",
    "        if worse_off_rec[n] == 4:\n",
    "            label_rec[n] = 1 # if node always is worse of\n",
    "        \n",
    "    return label_freq, label_rec\n",
    "\n",
    "def run_logReg_ML(X_,Y_):\n",
    "    # CV\n",
    "    kf = KFold(n_splits=5,shuffle=True)\n",
    "    \n",
    "    # balanced accuracy\n",
    "    accuracy = []\n",
    "    weights = []\n",
    "    \n",
    "    # find optimal parameters\n",
    "    for lambda_ in np.logspace(-5,5,201):\n",
    "        #tmp = []\n",
    "        #c = Counter()\n",
    "        #for train_index, test_index in skf.split(X_, Y_):\n",
    "        for train_index, test_index in kf.split(X_):\n",
    "            X_train, X_test = X_[train_index], X_[test_index]\n",
    "            Y_train, Y_test = Y_[train_index], Y_[test_index]\n",
    "\n",
    "            # standardize features\n",
    "            mean = np.mean(X_train,axis=0)\n",
    "            std = np.std(X_train,axis=0,ddof=1)\n",
    "            X_train = (X_train - mean)/std\n",
    "            X_test = (X_test - mean)/std\n",
    "\n",
    "            model = LogisticRegression(penalty='l2', tol=0.0001, C=lambda_, fit_intercept=True, \n",
    "                                    max_iter=1000000, n_jobs=-1, class_weight = 'balanced')\n",
    "\n",
    "\n",
    "            model.fit(X_train,Y_train)\n",
    "\n",
    "            accuracy.append(balanced_accuracy_score(Y_test, model.predict(X_test)))\n",
    "            weights.append(model.coef_)\n",
    "\n",
    "    return np.mean(accuracy), np.mean(weights,axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = []\n",
    "ML_coeff = []\n",
    "accuracy = []\n",
    "\n",
    "for idd_ in networks:\n",
    "    print(idd_)\n",
    "    # load network\n",
    "    G = load_network(network_path + idd_ + '.txt')\n",
    "    norm = 10.*G.number_of_nodes() # number of repetitions\n",
    "    seeds = max(1,int(round(len(G)/100.,0))) # select ~1% of nodes\n",
    "\n",
    "    # define measures\n",
    "    activation_count = dict()\n",
    "    activation_time = dict()\n",
    "\n",
    "    # load activation_count for the various methods\n",
    "    methods = ['random','HD','coreHD','degreeDiscount','kcore']\n",
    "    for method in methods:\n",
    "        activation_count[method] = dict()\n",
    "        with open(path_to_activation_count_data + '/%s_%s_seeds=%d_m=10N_p=pc.csv' % (idd_,method,seeds)) as f:\n",
    "            for line in f:\n",
    "                n,a = line.strip().split(',')\n",
    "                activation_count[method][int(n)] = int(a)/norm\n",
    "\n",
    "    # load activation_time for various methods\n",
    "    for method in methods:\n",
    "        activation_time[method] = dict()\n",
    "        with open(path_to_activation_time_data + '/%s_%s_seeds=%d_m=10N_p=pc.csv' % (idd_,method,seeds)) as f:\n",
    "            for line in f:\n",
    "                dat = line.strip().split(',')\n",
    "                n = dat[0]\n",
    "                # estimate inverse activation time\n",
    "                tau = 0.\n",
    "                for part_ in dat[1:]:\n",
    "                    t,nn = part_.split(':') # nn = number of times an epidemic spent t times reaching node\n",
    "                    tau += float(nn)/(float(t)+1)\n",
    "                activation_time[method][int(n)] = tau/norm\n",
    "    \n",
    "    label_freq, label_rec = label_nodes(activation_count,activation_time)\n",
    "    \n",
    "    if frequency:\n",
    "        # keep statistics\n",
    "        statistics.append((len(G), # total nodes\n",
    "                           len(label_freq)-sum(label_freq.values()), # label 0 nodes\n",
    "                           sum(label_freq.values()), # label 1 nodes\n",
    "                           idd_) # network id\n",
    "                         )\n",
    "    else:\n",
    "        # keep statistics\n",
    "        statistics.append((len(G), # total nodes\n",
    "                           len(label_rec)-sum(label_rec.values()), # label 0 nodes\n",
    "                           sum(label_rec.values()), # label 1 nodes\n",
    "                           idd_) # network id\n",
    "                         )\n",
    "        \n",
    "    # build feature matrix\n",
    "    features = ['degree','k-shell','clustering','betweenness','closeness','eigenvector','eccentricity']\n",
    "\n",
    "    degree = nx.degree(G)\n",
    "    k_shell = nx.core_number(G)\n",
    "    clustering = nx.clustering(G)\n",
    "    betweenness = load_measure(idd_,'betweenness')\n",
    "    closeness = load_measure(idd_,'closeness')\n",
    "    eigenvector = load_measure(idd_,'eigenvector')\n",
    "    eccentricity = load_measure(idd_,'eccentricity')\n",
    "    \n",
    "    # create freq dataset\n",
    "    X_freq = []\n",
    "    for n in sorted(label_freq.keys()):\n",
    "        X_freq.append([degree[n],k_shell[n],clustering[n],betweenness[n],closeness[n],eigenvector[n],eccentricity[n]])\n",
    "\n",
    "    X_freq = np.array(X_freq)\n",
    "    Y_freq = np.array([label_freq[n] for n in sorted(label_freq.keys())])\n",
    "    \n",
    "    # create rec dataset\n",
    "    X_rec = []\n",
    "    for n in sorted(label_rec.keys()):\n",
    "        X_rec.append([degree[n],k_shell[n],clustering[n],betweenness[n],closeness[n],eigenvector[n],eccentricity[n]])\n",
    "\n",
    "    X_rec = np.array(X_rec)\n",
    "    Y_rec = np.array([label_rec[n] for n in sorted(label_rec.keys())])\n",
    "    \n",
    "    # do some ML\n",
    "    if frequency:\n",
    "        accuracy_, coeff = run_logReg_ML(X_freq,Y_freq)\n",
    "    else:\n",
    "        accuracy_, coeff = run_logReg_ML(X_rec,Y_rec)\n",
    "\n",
    "    ML_coeff.append((idd_, #coeff)) # network id\n",
    "                    coeff/max(abs(coeff)))) # normalize with respect to largest value\n",
    "    \n",
    "    accuracy.append((idd_, # network id\n",
    "                     accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot some label statistics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = ['ia-infect-dublin','URVemail','polblogs','UC_irvine','facebook_combined','ca-CondMat','ca-AstroPh']\n",
    "network_labels = ['SocioPatterns','Emails (URV)','Political blogs','UCI messages','Facebook','CondMat','AstroPh']\n",
    "labels = dict(zip(networks,network_labels))\n",
    "\n",
    "# unpack data\n",
    "network_size, lab0, lab1, ids_ = zip(*sorted(statistics,key=operator.itemgetter(0)))\n",
    "\n",
    "# plot stuff\n",
    "plt.figure(figsize=(3,3))\n",
    "\n",
    "# label 0\n",
    "plt.plot(np.array(lab0)/(1.0*np.array(network_size)),color='#282828')\n",
    "plt.fill_between(range(len(network_size)),0,np.array(lab0)/(1.0*np.array(network_size)),color='#dcdcdc',alpha=1\n",
    "                 ,label='Label 0',lw=0)\n",
    "\n",
    "# label 1\n",
    "plt.fill_between(range(len(network_size)),np.array(lab0)/(1.0*np.array(network_size)),(np.array(lab0)+np.array(lab1))/(1.0*np.array(network_size)),\n",
    "                 color='orange',label='Label 1',lw=0)\n",
    "plt.plot((np.array(lab0)+np.array(lab1))/(1.0*np.array(network_size)),color='#282828')\n",
    "\n",
    "plt.legend(loc=0,frameon=True,fontsize=8)\n",
    "plt.xlim(0,4)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(range(len(ids_)),[labels.get(i,i) for i in ids_],fontsize=9,rotation=45,ha='right')\n",
    "#plt.xlabel('Network id')\n",
    "plt.ylabel('Fraction of nodes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot feature importance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = ['ia-infect-dublin','URVemail','polblogs','UC_irvine','facebook_combined','ca-CondMat','ca-AstroPh']\n",
    "network_labels = ['SocioPatterns','Emails (URV)','Political blogs','UCI messages','Facebook','CondMat','AstroPh']\n",
    "labels = dict(zip(networks,network_labels))\n",
    "plt.figure(figsize=(4,3.5))\n",
    "ids_, ml_coeff = zip(*ML_coeff)\n",
    "plt.imshow(np.array(ml_coeff).T,cmap=plt.cm.coolwarm,aspect='equal',vmin=-1,vmax=1) # LogReg\n",
    "cbar = plt.colorbar(orientation='vertical',pad=0.02,fraction=0.06)\n",
    "plt.xticks(range(len(ids_)),[labels[f] for f in ids_],fontsize=10,rotation=45,ha='right')\n",
    "plt.yticks(range(len(features)),features)\n",
    "cbar.set_label('relative feature weight')\n",
    "#plt.xlabel('Network id')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
